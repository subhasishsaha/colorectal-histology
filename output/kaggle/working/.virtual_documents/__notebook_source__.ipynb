


import tensorflow as tf
import tensorflow_datasets as tfds


(train_ds, val_ds, test_ds), metadata = tfds.load('colorectal_histology', split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'], with_info=True, as_supervised=True, batch_size = 32)


num_classes = metadata.features['label'].num_classes
print(f"Number of Classes: {num_classes}")


AUTOTUNE = tf.data.AUTOTUNE

train_ds = train_ds.shuffle(1000).prefetch(buffer_size=AUTOTUNE)


import matplotlib.pyplot as plt

images, labels = next(iter(train_ds))

images = images[:5]
labels = labels[:5]

plt.figure(figsize=(12, 4))
for i in range(5):
    plt.subplot(1, 5, i+1)
    plt.imshow(images[i].numpy().astype("uint8"))
    plt.title(f"Label: {labels[i].numpy()}")
    plt.axis("off")

plt.show()





for x in train_ds.take(1):
    print(x[0].shape)


def resize(image, label):
    image = tf.image.resize(image, [150, 150])
    image = tf.image.grayscale_to_rgb(image) if image.shape[-1] == 1 else image
    return tf.cast(image, tf.uint8), label

train_ds = train_ds.map(resize)


import matplotlib.pyplot as plt

for images, labels in train_ds.take(1):
    first_image = images[0].numpy()
    first_label = labels[0].numpy()
    break

plt.imshow(first_image.astype("uint8"))
plt.title(f"Label: {first_label}")
plt.axis('off')
plt.show()





def normalize_images(image, label):
    image = tf.cast(image, tf.float32) / 255.0
    return image, label

train_ds_2 = train_ds.map(normalize_images)
train_ds_1 = train_ds


import matplotlib.pyplot as plt

for images, labels in train_ds_2.take(1):
    first_image = images[0].numpy()
    first_label = labels[0].numpy()
    break

plt.imshow(first_image)
plt.title(f"Label: {first_label}")
plt.axis('off')
plt.show()





def augment(image, label):
    image = tf.image.random_flip_left_right(image)

    image = tf.image.rot90(image, tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32))

    image = tf.image.random_brightness(image, max_delta=0.2)
    image = tf.image.random_contrast(image, lower=0.8, upper=1.2)

    return image, label

for i in range(2):
    train_ds_2 = train_ds_2.concatenate(train_ds_2.map(augment))



total_images = 0
for image, label in train_ds_2.unbatch():
    total_images += 1

print(f"Total Images: {total_images}")


import matplotlib.pyplot as plt

for image, label in train_ds_2.take(1):
    image = image[0]
    label = label[0]
    plt.imshow(image.numpy()) 
    plt.title(f"Label: {str(label.numpy())}")          
    plt.axis("off")
    plt.show()






import tensorflow_datasets as tfds
import tensorflow as tf
import numpy as np
import cv2
import pandas as pd
import matplotlib.pyplot as plt

from skimage.filters import gabor
from skimage.feature import graycomatrix, graycoprops
from skimage import measure, morphology
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler 


from skimage.filters import gabor
from skimage.feature import graycomatrix, graycoprops
from skimage import morphology
from skimage.measure import regionprops
from scipy.stats import skew, kurtosis
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import numpy as np
import cv2

def gabor_features(gray, mask, out_size=(150, 150)):
    # Resize gray and mask
    gray_resized = cv2.resize(gray, out_size, interpolation=cv2.INTER_AREA)
    mask_resized = cv2.resize(mask.astype(np.uint8), out_size, interpolation=cv2.INTER_NEAREST).astype(bool)

    feats = []
    thetas = [0, np.pi/4, np.pi/2, 3*np.pi/4]
    freqs = [0.1, 0.2, 0.3]

    for theta in thetas:
        for freq in freqs:
            filt_real, _ = gabor(gray_resized, frequency=freq, theta=theta)
            # Normalize to [0,1] to avoid huge values
            filt_real = (filt_real - filt_real.min()) / (filt_real.max() - filt_real.min() + 1e-6)
            feats.append(filt_real)

    gabor_stack = np.stack(feats, axis=-1)

    roi_vals = gabor_stack[mask_resized]   # ROI only

    if roi_vals.size == 0:   # edge case: empty mask
        return {"gabor_mean": 0.0, "gabor_std": 0.0}

    per_filter_mean = roi_vals.mean(axis=0)
    per_filter_std = roi_vals.std(axis=0)

    return {
        "gabor_mean": float(np.mean(per_filter_mean)),
        "gabor_std": float(np.mean(per_filter_std)),
    }



def extract_features(mask, gray, label):
    """Extract shape + intensity + GLCM features from ROI."""
    props = regionprops(mask.astype(int), intensity_image=gray)
    if not props:
        return None

    region = props[0]

    # Make sure gray and mask are aligned
    if gray.shape != mask.shape:
        raise ValueError(f"Gray shape {gray.shape} != Mask shape {mask.shape}")

    features = {
        "label": int(label.numpy()),
        "area": region.area,
        "perimeter": region.perimeter,
        "circularity": (4 * np.pi * region.area) / (region.perimeter ** 2 + 1e-6),
        "eccentricity": region.eccentricity,
        "solidity": region.solidity,
        "extent": region.extent,
        "mean_intensity": region.mean_intensity,
        "std_intensity": np.std(gray[mask.astype(bool)]),
        "skewness": skew(gray[mask.astype(bool)].ravel()),
        "kurtosis": kurtosis(gray[mask.astype(bool)].ravel())
    }

    # üîπ GLCM texture features
    glcm = graycomatrix(gray, [1], [0], symmetric=True, normed=True)
    features.update({
        "contrast": graycoprops(glcm, 'contrast')[0, 0],
        "homogeneity": graycoprops(glcm, 'homogeneity')[0, 0],
        "energy": graycoprops(glcm, 'energy')[0, 0],
        "correlation": graycoprops(glcm, 'correlation')[0, 0],
    })

    return features


def process_image(image, label, k_clusters=3, out_size=(150, 150)):
    img = image.numpy().astype(np.uint8)

    # Ensure 3 channels
    if img.ndim == 2:
        img = np.stack([img]*3, axis=-1)
    elif img.shape[-1] == 1:
        img = np.repeat(img, 3, axis=-1)

    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
    lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)

    # KMeans clustering on LAB
    H, W, _ = lab.shape
    features = lab.reshape(-1, 3).astype(np.float32)

    scaler = StandardScaler()
    features_scaled = scaler.fit_transform(features)

    km = KMeans(n_clusters=k_clusters, random_state=42, n_init=10)
    labels = km.fit_predict(features_scaled)
    segmented = labels.reshape(H, W)

    # ROI cluster = darkest average gray
    roi_cluster = np.argmin([np.mean(gray[segmented == i]) for i in range(k_clusters)])
    mask = (segmented == roi_cluster).astype(np.uint8)

    # Clean mask
    mask = morphology.remove_small_objects(mask.astype(bool), min_size=50)
    mask = mask.astype(np.uint8)

    # Force fixed size
    gray_resized = cv2.resize(gray, out_size, interpolation=cv2.INTER_AREA)
    mask_resized = cv2.resize(mask, out_size, interpolation=cv2.INTER_NEAREST).astype(bool)

    # Extract features
    feats = extract_features(mask_resized, gray_resized, label)
    if feats is None:
        return None

    # Add Gabor features
    gabor_stats = gabor_features(gray_resized, mask_resized)
    feats.update(gabor_stats)

    return feats


from tqdm import tqdm

def extract_from_dataset(dataset, k_clusters=3, limit=None):
    features = []
    
    # Try to get total for tqdm
    try:
        total = limit if limit is not None else len(dataset)
    except TypeError:
        total = None 
    
    for i, (image, label) in enumerate(tqdm(dataset, desc="Processing images", total=total)):
        if limit and i >= limit:
            break
        try:
            feats = process_image(image, label, k_clusters=k_clusters)
            if feats is not None:
                # Add image name in front (IMG_001, IMG_002, ...)
                feats_with_name = {"image_name": f"IMG_{i+1:03d}"}
                feats_with_name.update(feats)
                features.append(feats_with_name)
        except Exception as e:
            print(f"Error processing image {i}: {e}")
            
    return features



# train_features = extract_from_dataset(train_ds_1.unbatch(), k_clusters=3, limit=None) 
# df = pd.DataFrame(train_features)


# df.head(10)


# df.to_csv("colorectal_histology_features.csv", index=False)
# print("‚úÖ Feature extraction complete. CSV saved.")





label_dict = {
    0: 'tumour epithelium',
    1: 'simple stroma',
    2: 'complex stroma',
    3: 'immune cell conglomerates',
    4: 'debris and mucus',
    5: 'mucosal glands',
    6: 'adipose tissue',
    7: 'background'
}


super_class_dict = {
    0: "tumour-related",
    1: "immune",
    2: "structural",
    3: "normal",
    4: "unknown"
}


def super_class(label):
    match label:
        case 0 | 1 | 2:
            category = 0
        case 3:
            category = 1
        case 4 | 7:
            category = 2
        case 5 | 6:
            category = 3
        case _:
            category = 4

    return category


import pandas as pd

df = pd.read_csv("/kaggle/input/colorectal-histology-ds/colorectal_histology_features_2.csv")
df.head()


df["super_class"] = df["label"].apply(super_class)
df.head()


import matplotlib.pyplot as plt

df["super_class"].value_counts().sort_index().plot(kind="bar")
plt.xticks(rotation=45)
plt.title("Distribution of Super Classes")
plt.xlabel("Super Class")
plt.ylabel("Count")
plt.show()


import numpy as np
from sklearn.utils.class_weight import compute_class_weight

classes = np.unique(df["super_class"])
class_weights = compute_class_weight(
    class_weight='balanced',
    classes=classes,
    y=df["super_class"]
)

class_weights_dict = dict(zip(classes, class_weights))





X = df.iloc[:, 2: -1]
y = df.iloc[:, -1]

classes = list(y.unique())


from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)


from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler

pipe = make_pipeline(StandardScaler())

steps = list(pipe.steps)





from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline


rf_steps = steps.copy()
rf_steps.append(("rf", RandomForestClassifier(n_jobs = -1, n_estimators = 1000, class_weight = class_weights_dict)))

rf_pipe = Pipeline(rf_steps)

rf_pipe.fit(X_train, y_train)


accuracy = rf_pipe.score(X_test, y_test)
print(f"Accuracy: {accuracy*100:.2f}%")


from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

y_preds = rf_pipe.predict(X_test)
cm = confusion_matrix(y_test, y_preds)
disp = ConfusionMatrixDisplay(cm)
disp.plot(cmap = "Greens")
plt.title("Heirarchial Model Prediction(Random Forest)")
plt.show()





from lightgbm import LGBMClassifier

lgbm_steps = steps.copy()

lgbm_steps.append((
    "lgbm", 
    LGBMClassifier(
        objective='multiclass',
        num_class=len(classes),
        n_estimators=500,
        learning_rate=0.05,
        n_jobs=-1, class_weight = class_weights_dict
    )
))


lgbm_pipe = Pipeline(lgbm_steps)
lgbm_pipe.fit(X_train, y_train)


accuracy = lgbm_pipe.score(X_test, y_test)
print(f"Accuracy: {accuracy*100:.2f}%")





from catboost import CatBoostClassifier

cat_steps = steps.copy()

cat_steps.append((
    "catboost",
    CatBoostClassifier(
        iterations=1000,
        depth=10,
        learning_rate=0.05,
        loss_function='MultiClass',
        verbose=0,
        random_seed=42, class_weights = class_weights_dict
    )
))

cat_pipe = Pipeline(cat_steps)
cat_pipe.fit(X_train, y_train)


print(f"Accuracy: {cat_pipe.score(X_test, y_test) * 100 :.2f}%")


import matplotlib.pyplot as plt
from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score

def compare_models_metrics(models: dict, X_test, y_test, save_path=None):
    """
    Compare multiple classification models on test data and plot metrics.

    Args:
        models (dict): Dictionary of {model_name: trained_model}.
        X_test (array-like): Test features.
        y_test (array-like): True labels.
        save_path (str, optional): Path to save the figure. Defaults to None.

    Returns:
        None
    """
    metrics = ['F1-score', 'Recall', 'Precision', 'Accuracy', 'Error']
    n_metrics = len(metrics)

    # Prepare figure
    fig, axes = plt.subplots(1, n_metrics, figsize=(5*n_metrics, 5))
    
    # Compute metrics per model
    results = {metric: [] for metric in metrics}
    
    for model_name, model in models.items():
        y_pred = model.predict(X_test)
        
        results['F1-score'].append(f1_score(y_test, y_pred, average='macro'))
        results['Recall'].append(recall_score(y_test, y_pred, average='macro'))
        results['Precision'].append(precision_score(y_test, y_pred, average='macro'))
        acc = accuracy_score(y_test, y_pred)
        results['Accuracy'].append(acc)
        results['Error'].append(1 - acc)
    
    # Plot each metric
    for i, metric in enumerate(metrics):
        axes[i].bar(models.keys(), results[metric], color=['skyblue', 'salmon', 'lightgreen'])
        axes[i].set_title(metric)
        axes[i].set_ylim(0, 1)
        for j, val in enumerate(results[metric]):
            axes[i].text(j, val + 0.02, f"{val:.2f}", ha='center', fontsize=10)
        axes[i].set_xticklabels(models.keys(), rotation=45, ha='right')

    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, bbox_inches='tight', dpi=300)
    
    plt.show()



models = {"Random Forest":rf_pipe, "CatBoost":cat_pipe, "LightGBM":lgbm_pipe}

compare_models_metrics(models, X_test, y_test, "/kaggle/working/heirarchial.png")


import copy
first_model = copy.deepcopy(lgbm_pipe)


import joblib
joblib.dump(first_model, "heirarchial_model.pkl")


hierarchial_model = joblib.load("/kaggle/working/heirarchial_model.pkl")


from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

y_preds = hierarchial_model.predict(X_test)
cm = confusion_matrix(y_test, y_preds)
disp = ConfusionMatrixDisplay(cm)
disp.plot(cmap = "Greens")
plt.title("Heirarchial Model Prediction")

plt.savefig("/kaggle/working/heirarchial_cm.png")
plt.show()





df_tumour = df[df["super_class"] == 0]
df_tumour.head()


df_tumour["label"].value_counts().sort_index().plot(kind = "bar")
plt.xticks(rotation = 45)
plt.title("Distribution of Tumour Classes")
plt.ylabel("Count")
plt.xlabel("Tumour Types")
plt.show()


X, y = df_tumour.iloc[:, 2: -1], df_tumour.iloc[:, 1]
X.head()


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)


from sklearn.decomposition import PCA

pipe = make_pipeline(StandardScaler(), PCA(0.95))
steps = list(pipe.steps)





from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline

rf_steps = steps.copy()

rf_steps.append(("rf", RandomForestClassifier(n_jobs = -1, n_estimators = 1000)))

rf_tumour_pipe = Pipeline(rf_steps)

rf_tumour_pipe.fit(X_train, y_train)


accuracy = rf_tumour_pipe.score(X_test, y_test)
print(f"Accuracy: {accuracy*100:.2f}%")





from catboost import CatBoostClassifier

cat_steps = steps.copy()

cat_steps.append((
    "catboost",
    CatBoostClassifier(
        iterations=1000,
        depth=10,
        learning_rate=0.05,
        loss_function='MultiClass',
        verbose=0,
        random_seed=42
    )
))

cat_tumour_pipe = Pipeline(cat_steps)
cat_tumour_pipe.fit(X_train, y_train)


print(f"Accuracy: {cat_tumour_pipe.score(X_test, y_test) * 100 :.2f}%")





from lightgbm import LGBMClassifier

lgbm_steps = steps.copy()

lgbm_steps.append((
    "lgbm", 
    LGBMClassifier(
        objective='multiclass',
        num_class=len(classes),
        n_estimators=500,
        learning_rate=0.05,
        n_jobs=-1
    )
))


lgbm_tumour_pipe = Pipeline(lgbm_steps)
lgbm_tumour_pipe.fit(X_train, y_train)


accuracy = lgbm_tumour_pipe.score(X_test, y_test)
print(f"Accuracy: {accuracy*100:.2f}%")


tumour_model = copy.deepcopy(cat_tumour_pipe)


joblib.dump(tumour_model, "tumour_model.pkl")


tumour_model = joblib.load("/kaggle/working/tumour_model.pkl")

y_preds = tumour_model.predict(X_test)
y_preds = y_preds.reshape(y_preds.shape[0])
cm = confusion_matrix(y_test, y_preds)
disp = ConfusionMatrixDisplay(cm)
disp.plot(cmap = "Greens")
plt.title("Tumour Model Prediction")
plt.savefig("/kaggle/working/tumour_cm.png")
plt.show()


models = {"Random Forest":rf_tumour_pipe, "CatBoost":cat_tumour_pipe, "LightGBM":lgbm_tumour_pipe}

compare_models_metrics(models, X_test, y_test, "/kaggle/working/tumour.png")





df_structural = df[df["super_class"] == 2]
df_structural.head()


df_structural["label"].value_counts().sort_index().plot(kind = "bar")
plt.xticks(rotation = 45)
plt.title("Distribution of Structural Classes")
plt.ylabel("Count")
plt.xlabel("Structural Types")
plt.show()


X, y = df_structural.iloc[:, 2: -1], df_structural.iloc[:, 1]
X.head()


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)


from sklearn.utils.class_weight import compute_class_weight

classes_struc = np.unique(y_train)
class_weights_struc = compute_class_weight(
    class_weight='balanced',
    classes=classes_struc,
    y=y_train
)

class_weights_dict_struc = dict(zip(classes_struc, class_weights_struc))


pipe = make_pipeline(StandardScaler())
steps = list(pipe.steps)





from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline

rf_steps = steps.copy()

rf_steps.append(("rf", RandomForestClassifier(n_jobs = -1, n_estimators = 100, class_weight = class_weights_dict_struc)))

rf_structural_pipe = Pipeline(rf_steps)

rf_structural_pipe.fit(X_train, y_train)


accuracy = rf_structural_pipe.score(X_test, y_test)
print(f"Accuracy: {accuracy*100:.2f}%")





from lightgbm import LGBMClassifier

lgbm_steps = steps.copy()

lgbm_steps.append((
    "lgbm", 
    LGBMClassifier(
        objective='multiclass',
        num_class=len(classes),
        n_estimators=500,
        learning_rate=0.05,
        n_jobs=-1, class_weight = class_weights_dict_struc
    )
))


lgbm_structural_pipe = Pipeline(lgbm_steps)
lgbm_structural_pipe.fit(X_train, y_train)


accuracy = lgbm_structural_pipe.score(X_test, y_test)
print(f"Accuracy: {accuracy*100:.2f}%")





from catboost import CatBoostClassifier

cat_steps = steps.copy()

cat_steps.append((
    "catboost",
    CatBoostClassifier(
        iterations=1000,
        depth=10,
        learning_rate=0.05,
        loss_function='MultiClass',
        verbose=0,
        random_seed=42, class_weights = class_weights_dict_struc
    )
))

cat_structural_pipe = Pipeline(cat_steps)
cat_structural_pipe.fit(X_train, y_train)


print(f"Accuracy: {cat_structural_pipe.score(X_test, y_test) * 100 :.2f}%")


joblib.dump( rf_structural_pipe, "structural_model.pkl")


structural_model = joblib.load("/kaggle/working/structural_model.pkl")

y_preds = structural_model.predict(X_test)

cm = confusion_matrix(y_test, y_preds)
disp = ConfusionMatrixDisplay(cm)
disp.plot(cmap= "Greens")
plt.title("Structural Model Prediction")
plt.savefig("/kaggle/working/structural_cm.png")
plt.show()


models = {"Random Forest":rf_structural_pipe, "CatBoost":cat_structural_pipe, "LightGBM":lgbm_structural_pipe}

compare_models_metrics(models, X_test, y_test, "/kaggle/working/structural.png")





df_normal = df[df["super_class"] == 3]
df_normal.head()


df_normal["label"].value_counts().sort_index().plot(kind = "bar")
plt.xticks(rotation = 45)
plt.title("Distribution of Normal Classes")
plt.ylabel("Count")
plt.xlabel("Normal Types")
plt.show()


X, y = df_normal.iloc[:, 2: -1], df_normal.iloc[:, 1]
X.head()


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)


from sklearn.decomposition import PCA

pipe = make_pipeline(StandardScaler(),PCA(0.95))
steps = list(pipe.steps)





from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline

rf_normal_steps = steps.copy()

rf_normal_steps.append(("rf", RandomForestClassifier(n_jobs = -1, n_estimators = 1000)))

rf_normal_pipe = Pipeline(rf_normal_steps)

rf_normal_pipe.fit(X_train, y_train)


accuracy = rf_normal_pipe.score(X_test, y_test)
print(f"Accuracy: {accuracy*100:.2f}%")





from lightgbm import LGBMClassifier

lgbm_steps = steps.copy()

lgbm_steps.append((
    "lgbm", 
    LGBMClassifier(
        objective='multiclass',
        num_class=len(classes),
        n_estimators=500,
        learning_rate=0.05,
        n_jobs=-1
    )
))


lgbm_normal_pipe = Pipeline(lgbm_steps)
lgbm_normal_pipe.fit(X_train, y_train)


accuracy = lgbm_normal_pipe.score(X_test, y_test)
print(f"Accuracy: {accuracy*100:.2f}%")





from catboost import CatBoostClassifier

cat_steps = steps.copy()

cat_steps.append((
    "catboost",
    CatBoostClassifier(
        iterations=1000,
        depth=10,
        learning_rate=0.05,
        loss_function='MultiClass',
        verbose=0,
        random_seed=42
    )
))

cat_normal_pipe = Pipeline(cat_steps)
cat_normal_pipe.fit(X_train, y_train)


print(f"Accuracy: {cat_normal_pipe.score(X_test, y_test) * 100 :.2f}%")


joblib.dump(rf_normal_pipe, "normal_model.pkl")


normal_model = joblib.load("/kaggle/working/normal_model.pkl")

y_preds = normal_model.predict(X_test)
cm = confusion_matrix(y_test, y_preds)
disp = ConfusionMatrixDisplay(cm)
disp.plot(cmap = "Greens")
plt.title("Normal Model Prediction")
plt.savefig("/kaggle/working/normal_cm.png")
plt.show()


models = {"Random Forest":rf_normal_pipe, "CatBoost":cat_normal_pipe, "LightGBM":lgbm_normal_pipe}

compare_models_metrics(models, X_test, y_test, "/kaggle/working/normal.png")


get_ipython().getoutput("zip -r /kaggle/working/output.zip /kaggle/working/")


# pip install gradio


# # ===========================
# # üì¶ Imports
# # ===========================
# import gradio as gr
# import pickle
# import numpy as np
# from PIL import Image
# from torchvision import transforms
# import joblib
# import pandas as pd

# # ===========================
# # ‚öôÔ∏è Load Models
# # ===========================
# # Update paths according to your Kaggle input directory
# MODEL_PATH = "/kaggle/input/your-models"

# hybrid_model = joblib.load("/kaggle/input/colorectal-histology-model/scikitlearn/default/1/kaggle/working/heirarchial_model.pkl")

# tumour_model = joblib.load("/kaggle/input/colorectal-histology-model/scikitlearn/default/1/kaggle/working/tumour_model.pkl")

# structural_model = joblib.load("/kaggle/input/colorectal-histology-model/scikitlearn/default/1/kaggle/working/structural_model.pkl")

# normal_model = joblib.load("/kaggle/input/colorectal-histology-model/scikitlearn/default/1/kaggle/working/normal_model.pkl")

# # ===========================
# # üß† Dictionaries
# # ===========================
# super_class_dict = {
#     0: "tumour-related",
#     1: "immune",
#     2: "structural",
#     3: "normal",
#     4: "unknown"
# }

# label_dict = {
#     0: 'tumour epithelium',
#     1: 'simple stroma',
#     2: 'complex stroma',
#     3: 'immune cell conglomerates',
#     4: 'debris and mucus',
#     5: 'mucosal glands',
#     6: 'adipose tissue',
#     7: 'background'
# }

# def super_class(label):
#     match label:
#         case 0 | 1 | 2:
#             category = 0
#         case 3:
#             category = 1
#         case 4 | 7:
#             category = 2
#         case 5 | 6:
#             category = 3
#         case _:
#             category = 4
#     return category

# # ===========================
# # üß© Feature Extraction
# # ===========================
# import numpy as np
# import cv2
# from skimage.filters import gabor
# from skimage.feature import graycomatrix, graycoprops
# from skimage import morphology
# from skimage.measure import regionprops
# from scipy.stats import skew, kurtosis
# from sklearn.preprocessing import StandardScaler
# from sklearn.cluster import KMeans

# def extract_features_single(image_pil, k_clusters=3, out_size=(150, 150)):
#     """
#     Extract the same handcrafted features for a single user-uploaded PIL image.
#     Returns a numpy feature vector (1, N) ready for model prediction.
#     """
#     # Convert PIL to numpy
#     img = np.array(image_pil)

#     # Ensure correct channel order
#     if img.ndim == 2:  # grayscale
#         img = np.stack([img]*3, axis=-1)
#     elif img.shape[-1] == 4:  # RGBA ‚Üí RGB
#         img = cv2.cvtColor(img, cv2.COLOR_RGBA2RGB)

#     gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
#     lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)

#     # KMeans segmentation (same as training)
#     H, W, _ = lab.shape
#     features = lab.reshape(-1, 3).astype(np.float32)
#     scaler = StandardScaler()
#     features_scaled = scaler.fit_transform(features)

#     km = KMeans(n_clusters=k_clusters, random_state=42, n_init=10)
#     labels = km.fit_predict(features_scaled)
#     segmented = labels.reshape(H, W)

#     # Select ROI = darkest average gray region
#     roi_cluster = np.argmin([np.mean(gray[segmented == i]) for i in range(k_clusters)])
#     mask = (segmented == roi_cluster).astype(np.uint8)
#     mask = morphology.remove_small_objects(mask.astype(bool), min_size=50)
#     mask = mask.astype(np.uint8)

#     # Resize for consistency
#     gray_resized = cv2.resize(gray, out_size, interpolation=cv2.INTER_AREA)
#     mask_resized = cv2.resize(mask, out_size, interpolation=cv2.INTER_NEAREST).astype(bool)

#     # ---- Region features ----
#     props = regionprops(mask_resized.astype(int), intensity_image=gray_resized)
#     if not props:
#         raise ValueError("No region found in mask.")
#     region = props[0]

#     features_dict = {
#         "area": region.area,
#         "perimeter": region.perimeter,
#         "circularity": (4 * np.pi * region.area) / (region.perimeter ** 2 + 1e-6),
#         "eccentricity": region.eccentricity,
#         "solidity": region.solidity,
#         "extent": region.extent,
#         "mean_intensity": region.mean_intensity,
#         "std_intensity": np.std(gray_resized[mask_resized]),
#         "skewness": skew(gray_resized[mask_resized].ravel()),
#         "kurtosis": kurtosis(gray_resized[mask_resized].ravel())
#     }

#     # ---- Texture features (GLCM) ----
#     glcm = graycomatrix(gray_resized, [1], [0], symmetric=True, normed=True)
#     features_dict.update({
#         "contrast": graycoprops(glcm, 'contrast')[0, 0],
#         "homogeneity": graycoprops(glcm, 'homogeneity')[0, 0],
#         "energy": graycoprops(glcm, 'energy')[0, 0],
#         "correlation": graycoprops(glcm, 'correlation')[0, 0],
#     })

#     # ---- Gabor features ----
#     thetas = [0, np.pi/4, np.pi/2, 3*np.pi/4]
#     freqs = [0.1, 0.2, 0.3]
#     feats = []
#     for theta in thetas:
#         for freq in freqs:
#             filt_real, _ = gabor(gray_resized, frequency=freq, theta=theta)
#             filt_real = (filt_real - filt_real.min()) / (filt_real.max() - filt_real.min() + 1e-6)
#             feats.append(filt_real)
#     gabor_stack = np.stack(feats, axis=-1)
#     roi_vals = gabor_stack[mask_resized]
#     per_filter_mean = roi_vals.mean(axis=0)
#     per_filter_std = roi_vals.std(axis=0)
#     features_dict.update({
#         "gabor_mean": float(np.mean(per_filter_mean)),
#         "gabor_std": float(np.mean(per_filter_std))
#     })

#     # ---- Convert to vector (1 x N) ----
#     feature_vector = np.array(list(features_dict.values()), dtype=np.float32).reshape(1, -1)
#     return feature_vector



# # ===========================
# # üöÄ Prediction Logic
# # ===========================
# def predict(image):
#     # Extract features
#     try:
#         image_features =  extract_features_single(image)
#     except:
#         return "unknown", "unknown"
#     features=pd.DataFrame(image_features)
#     # Predict superclass
#     super_pred = hybrid_model.predict(features)[0]
#     superclass_name = super_class_dict.get(super_pred, "unknown")
#     print(superclass_name)

#     # Based on superclass, pick correct model
#     if super_pred == 0:  # tumour-related
#         sub_pred = tumour_model.predict(features)[0][0]
#         print(sub_pred)
#         label_name = label_dict.get(sub_pred,"unknown")

#     elif super_pred == 1:  # immune
#         label_name = "immune cell conglomerates"

#     elif super_pred == 2:  # structural
#         sub_pred = structural_model.predict(features)[0]
#         label_name = label_dict.get(sub_pred,"unknown")

#     elif super_pred == 3:  # normal
#         sub_pred = normal_model.predict(features)[0]
#         label_name = label_dict.get(sub_pred,"unknown")

#     else:  # unknown
#         label_name = "unknown"

#     return superclass_name, label_name

# # ===========================
# # üé® Gradio UI
# # ===========================
# demo = gr.Interface(
#     fn=predict,
#     inputs=gr.Image(type="pil", label="Upload Histopathology Image"),
#     outputs=[
#         gr.Label(label="Superclass Prediction"),
#         gr.Label(label="Detailed Label Prediction")
#     ],
#     title="üß¨ Hybrid Image Classifier",
#     description=(
#         "Uploads an image ‚Üí Extracts features ‚Üí Predicts superclass (0‚Äì4) "
#         "and then uses the corresponding submodel for fine-grained classification."
#     )
# )

# # ===========================
# # ‚ñ∂Ô∏è Launch
# # ===========================
# demo.launch()


# import tensorflow as tf
# import matplotlib.pyplot as plt
# import os

# os.makedirs("saved_test_images", exist_ok=True)

# # Iterate over dataset
# for i, (image, label) in enumerate(test_ds.unbatch().take(30)): 
#     img = tf.image.convert_image_dtype(image, dtype=tf.uint8).numpy()
    
#     plt.imsave(f"saved_test_images/test_img_{i+1}.png", img)
    
#     print(f"Saved test_img_{i+1}.png (label: {label.numpy()})")

# print("‚úÖ Done saving sample images!")





# # ===========================
# # üì¶ Imports
# # ===========================
# import joblib
# import numpy as np
# import pandas as pd
# import cv2
# from skimage.filters import gabor
# from skimage.feature import graycomatrix, graycoprops
# from skimage import morphology
# from skimage.measure import regionprops
# from scipy.stats import skew, kurtosis
# from sklearn.preprocessing import StandardScaler
# from sklearn.cluster import KMeans

# # ===========================
# # ‚öôÔ∏è Load Models
# # ===========================
# hybrid_model = joblib.load("/kaggle/input/colorectal-histology-model/scikitlearn/default/1/kaggle/working/heirarchial_model.pkl")
# tumour_model = joblib.load("/kaggle/input/colorectal-histology-model/scikitlearn/default/1/kaggle/working/tumour_model.pkl")
# structural_model = joblib.load("/kaggle/input/colorectal-histology-model/scikitlearn/default/1/kaggle/working/structural_model.pkl")
# normal_model = joblib.load("/kaggle/input/colorectal-histology-model/scikitlearn/default/1/kaggle/working/normal_model.pkl")

# # ===========================
# # üß† Dictionaries
# # ===========================
# super_class_dict = {
#     0: "tumour-related",
#     1: "immune",
#     2: "structural",
#     3: "normal",
#     4: "unknown"
# }

# label_dict = {
#     0: 'tumour epithelium',
#     1: 'simple stroma',
#     2: 'complex stroma',
#     3: 'immune cell conglomerates',
#     4: 'debris and mucus',
#     5: 'mucosal glands',
#     6: 'adipose tissue',
#     7: 'background'
# }

# # ===========================
# # üß© Feature Extraction
# # ===========================
# def extract_features_single(image_pil, k_clusters=3, out_size=(150, 150)):
#     img = np.array(image_pil)
#     if img.ndim == 2:
#         img = np.stack([img]*3, axis=-1)
#     elif img.shape[-1] == 4:
#         img = cv2.cvtColor(img, cv2.COLOR_RGBA2RGB)

#     gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
#     lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)

#     # KMeans segmentation
#     H, W, _ = lab.shape
#     features = lab.reshape(-1, 3).astype(np.float32)
#     scaler = StandardScaler()
#     features_scaled = scaler.fit_transform(features)

#     km = KMeans(n_clusters=k_clusters, random_state=42, n_init=10)
#     labels = km.fit_predict(features_scaled)
#     segmented = labels.reshape(H, W)

#     roi_cluster = np.argmin([np.mean(gray[segmented == i]) for i in range(k_clusters)])
#     mask = (segmented == roi_cluster).astype(np.uint8)
#     mask = morphology.remove_small_objects(mask.astype(bool), min_size=50).astype(np.uint8)

#     gray_resized = cv2.resize(gray, out_size, interpolation=cv2.INTER_AREA)
#     mask_resized = cv2.resize(mask, out_size, interpolation=cv2.INTER_NEAREST).astype(bool)

#     props = regionprops(mask_resized.astype(int), intensity_image=gray_resized)
#     if not props:
#         raise ValueError("No region found in mask.")
#     region = props[0]

#     features_dict = {
#         "area": region.area,
#         "perimeter": region.perimeter,
#         "circularity": (4 * np.pi * region.area) / (region.perimeter ** 2 + 1e-6),
#         "eccentricity": region.eccentricity,
#         "solidity": region.solidity,
#         "extent": region.extent,
#         "mean_intensity": region.mean_intensity,
#         "std_intensity": np.std(gray_resized[mask_resized]),
#         "skewness": skew(gray_resized[mask_resized].ravel()),
#         "kurtosis": kurtosis(gray_resized[mask_resized].ravel())
#     }

#     glcm = graycomatrix(gray_resized, [1], [0], symmetric=True, normed=True)
#     features_dict.update({
#         "contrast": graycoprops(glcm, 'contrast')[0, 0],
#         "homogeneity": graycoprops(glcm, 'homogeneity')[0, 0],
#         "energy": graycoprops(glcm, 'energy')[0, 0],
#         "correlation": graycoprops(glcm, 'correlation')[0, 0],
#     })

#     thetas = [0, np.pi/4, np.pi/2, 3*np.pi/4]
#     freqs = [0.1, 0.2, 0.3]
#     feats = []
#     for theta in thetas:
#         for freq in freqs:
#             filt_real, _ = gabor(gray_resized, frequency=freq, theta=theta)
#             filt_real = (filt_real - filt_real.min()) / (filt_real.max() - filt_real.min() + 1e-6)
#             feats.append(filt_real)
#     gabor_stack = np.stack(feats, axis=-1)
#     roi_vals = gabor_stack[mask_resized]
#     per_filter_mean = roi_vals.mean(axis=0)
#     per_filter_std = roi_vals.std(axis=0)
#     features_dict.update({
#         "gabor_mean": float(np.mean(per_filter_mean)),
#         "gabor_std": float(np.mean(per_filter_std))
#     })

#     feature_vector = np.array(list(features_dict.values()), dtype=np.float32).reshape(1, -1)
#     return feature_vector

# def extract_features_batch(images, k_clusters=3, out_size=(150, 150)):
#     feature_list = []
#     for img in images:
#         try:
#             feature_vec = extract_features_single(img, k_clusters=k_clusters, out_size=out_size)
#             feature_list.append(feature_vec)
#         except:
#             feature_list.append(np.zeros((1, len(extract_features_single(images[0])))))
#     feature_array = np.vstack(feature_list)
#     return pd.DataFrame(feature_array)

# # ===========================
# # üß© Superclass helper
# # ===========================
# def super_class(label):
#     if label in [0, 1, 2]:
#         return 0
#     elif label == 3:
#         return 1
#     elif label in [4, 7]:
#         return 2
#     elif label in [5, 6]:
#         return 3
#     else:
#         return 4

# # ===========================
# # üöÄ Pipeline Prediction
# # ===========================
# def pipeline_predict_numeric(test_ds):
#     test_images = []
#     y_test = []

#     # Collect images and true labels
#     for img, label in test_ds:
#         test_images.append(img)
#         y_test.append(label.numpy() if hasattr(label, 'numpy') else label)

#     # Extract features
#     test_features = extract_features_batch(test_images)

#     # Predict superclass
#     super_preds = hybrid_model.predict(test_features)

#     # Predict final numeric labels
#     y_preds = []
#     for i, super_pred in enumerate(super_preds):
#         features_row = test_features.iloc[[i]]  # keep as DataFrame
#         if super_pred == 0:
#             sub_pred = tumour_model.predict(features_row)[0]
#             y_preds.append(sub_pred)
#         elif super_pred == 1:
#             y_preds.append(3)  # immune ‚Üí label index 3
#         elif super_pred == 2:
#             sub_pred = structural_model.predict(features_row)[0]
#             y_preds.append(sub_pred)
#         elif super_pred == 3:
#             sub_pred = normal_model.predict(features_row)[0]
#             y_preds.append(sub_pred)
#         else:
#             y_preds.append(-1)  # unknown

#     y_test = np.array(y_test).reshape(-1)
#     y_preds = np.array(y_preds).reshape(-1)

#     return y_test, y_preds



# y_test, y_preds = pipeline_predict_numeric(test_ds)



